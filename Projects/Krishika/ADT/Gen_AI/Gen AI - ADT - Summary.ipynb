{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d02dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e607cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/Documents/Factihealth/Forecast_Data.csv')\n",
    "\n",
    "actual_df = data[:12]\n",
    "predicted_df = data[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a14cb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 12/20/2020 the actual count of admissions were 95, actual count of discharges were 68 and actual count of transfers were 7.\n",
      "On 12/21/2020 the actual count of admissions were 67, actual count of discharges were 77 and actual count of transfers were 3.\n",
      "On 12/22/2020 the actual count of admissions were 88, actual count of discharges were 83 and actual count of transfers were 5.\n",
      "On 12/23/2020 the actual count of admissions were 106, actual count of discharges were 65 and actual count of transfers were 4.\n",
      "On 12/24/2020 the actual count of admissions were 96, actual count of discharges were 84 and actual count of transfers were 5.\n",
      "On 12/25/2020 the actual count of admissions were 104, actual count of discharges were 69 and actual count of transfers were 3.\n",
      "On 12/26/2020 the actual count of admissions were 98, actual count of discharges were 79 and actual count of transfers were 8.\n",
      "On 12/27/2020 the actual count of admissions were 87, actual count of discharges were 89 and actual count of transfers were 6.\n",
      "On 12/28/2020 the actual count of admissions were 105, actual count of discharges were 83 and actual count of transfers were 6.\n",
      "On 12/29/2020 the actual count of admissions were 229, actual count of discharges were 59 and actual count of transfers were 7.\n",
      "On 12/30/2020 the actual count of admissions were 231, actual count of discharges were 60 and actual count of transfers were 6.\n",
      "On 12/31/2020 the actual count of admissions were 237, actual count of discharges were 69 and actual count of transfers were 7.\n",
      "On 1/1/2021 the predicted count of admissions is 202, predicted count of discharges is 71 and predicted count of transfers is 7.\n",
      "On 1/2/2021 the predicted count of admissions is 182, predicted count of discharges is 70 and predicted count of transfers is 6.\n",
      "On 1/3/2021 the predicted count of admissions is 207, predicted count of discharges is 75 and predicted count of transfers is 7.\n",
      "On 1/4/2021 the predicted count of admissions is 170, predicted count of discharges is 58 and predicted count of transfers is 6.\n",
      "On 1/5/2021 the predicted count of admissions is 174, predicted count of discharges is 65 and predicted count of transfers is 6.\n",
      "On 1/6/2021 the predicted count of admissions is 201, predicted count of discharges is 66 and predicted count of transfers is 7.\n",
      "On 1/7/2021 the predicted count of admissions is 226, predicted count of discharges is 61 and predicted count of transfers is 7.\n",
      "On 1/8/2021 the predicted count of admissions is 194, predicted count of discharges is 75 and predicted count of transfers is 7.\n",
      "On 1/9/2021 the predicted count of admissions is 176, predicted count of discharges is 67 and predicted count of transfers is 6.\n",
      "On 1/10/2021 the predicted count of admissions is 331, predicted count of discharges is 82 and predicted count of transfers is 7.\n",
      "On 1/11/2021 the predicted count of admissions is 339, predicted count of discharges is 89 and predicted count of transfers is 6.\n",
      "On 1/12/2021 the predicted count of admissions is 350, predicted count of discharges is 98 and predicted count of transfers is 7.\n",
      "On 1/13/2021 the predicted count of admissions is 300, predicted count of discharges is 99 and predicted count of transfers is 7.\n",
      "On 1/14/2021 the predicted count of admissions is 260, predicted count of discharges is 101 and predicted count of transfers is 6.\n",
      "On 1/15/2021 the predicted count of admissions is 289, predicted count of discharges is 107 and predicted count of transfers is 7.\n"
     ]
    }
   ],
   "source": [
    "actual_data_string = \"\\n\".join(\n",
    "    f\"On {row['Date']} the actual count of admissions were {row['Admissions_Patient_Count']}, actual count of discharges were {row['Discharge_Patient_Count']} and actual count of transfers were {row['Transfer_Patient_Count']}.\"\n",
    "    for index, row in actual_df.iterrows())\n",
    "print(actual_data_string)\n",
    "\n",
    "predicted_data_string = \"\\n\".join(\n",
    "    f\"On {row['Date']} the predicted count of admissions is {row['Admissions_Patient_Count']}, predicted count of discharges is {row['Discharge_Patient_Count']} and predicted count of transfers is {row['Transfer_Patient_Count']}.\"\n",
    "    for index, row in predicted_df.iterrows()\n",
    ")\n",
    "print(predicted_data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5efa5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "Imagine you are a Hospital Administrator, adept in the dynamic and challenging environment of healthcare management. Your primary role\n",
    "revolves around scrutinizing a predictive dashboard, which provides a comprehensive view of the last 10 days' actual and the next 10 days'\n",
    "forecasted patient admissions, discharges, and transfers. Equipped with strong analytical skills, you adeptly interpret these data trends\n",
    "to make critical operational decisions, ensuring optimal resource allocation and staffing efficiency. Your days are marked by coordinating\n",
    "with various departments, managing staffing schedules, and adjusting resources in response to fluctuating patient flows. With a background\n",
    "in healthcare administration and a keen understanding of healthcare systems and policies, you are well-versed in contingency planning,\n",
    "ready to adapt to unforeseen circumstances. Your leadership is defined by quick decision-making, effective communication, and a\n",
    "detail-oriented approach, ensuring the hospital operates smoothly and continues to provide high-quality patient care.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6a8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "You are tasked with extracting insights from the data received from the hospital's predictive dashboard. This critical role \n",
    "requires you to analyze the past 10 days' actual data and the next 7 days' forecasted\n",
    "data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
    "could significantly influence hospital operations. Provide through insights which involves not only a deep understanding of the data but \n",
    "also a foresighted approach to foresee potential challenges and opportunities it indicates. Your insights should focus on optimizing \n",
    "resource allocation, effectively managing staffing, and enhancing overall operational efficiency. Importantly, each insight must be \n",
    "accompanied by a justification, explaining why it is necessary and how it will positively impact the hospital's ability to provide \n",
    "high-quality patient care while adapting to the dynamic healthcare environment. Need this in json format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e99d9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"\"\"\n",
    "Am a hospital administrator. I have patients count for Admissions, Discharges and Transfers. \n",
    "The {actual_data_string} is the past data and {predicted_data_string} is the forecasted data for Admissions, Discharges and Transfers of my hospital.\n",
    "Based on the {predicted_data_string} I need to generate Insights with specific graphs or data points. \n",
    "This data will be consumed for hospital staffing, bed utilization and other hospital administration purposes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0152d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5cced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"\"\"{persona}\n",
    "    {context}\n",
    "    {task}\"\"\"\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"\"\"Here is the data from the dashboard\n",
    "{actual_data_string}\n",
    "{predicted_data_string}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6dcf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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' #'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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150e1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddce879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m chat_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m      2\u001b[0m     [system_message_prompt, human_message_prompt]\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# get a chat completion from the formatted messages\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersona\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_data_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactual_data_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_data_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredicted_data_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\base.py:600\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    599\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 600\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    601\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    602\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    348\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    350\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    351\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    353\u001b[0m ]\n\u001b[0;32m    354\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 339\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    340\u001b[0m                 m,\n\u001b[0;32m    341\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    342\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    343\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    344\u001b[0m             )\n\u001b[0;32m    345\u001b[0m         )\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    493\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\openai.py:422\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    421\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 422\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    423\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    424\u001b[0m )\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\openai.py:352\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\chat_models\\openai.py:350\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "response = chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        persona=persona, task=task, actual_data_string=actual_data_string, predicted_data_string=predicted_data_string,\n",
    "        context = context\n",
    "    ).to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff042183",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response.content\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166b388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_data_string = \"\\n\".join(\n",
    "    f\"On {row['Date']} the predicted count of admissions is {row['Admissions_Patient_Count']}, predicted count of discharges is {row['Discharge_Patient_Count']} and predicted count of transfers is {row['Transfer_Patient_Count']}.\"\n",
    "    for index, row in predicted_df.iterrows()\n",
    ")\n",
    "print(predicted_data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9965c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "Imagine you are a Hospital Administrator, adept in the dynamic and challenging environment of healthcare management. Your primary role\n",
    "revolves around scrutinizing a predictive dashboard, which provides a comprehensive view of the data provided forecasted patient admissions, discharges, and transfers. Equipped with strong analytical skills, you adeptly interpret these data trends\n",
    "to make critical operational decisions, ensuring optimal resource allocation and staffing efficiency. Your days are marked by coordinating\n",
    "with various departments, managing staffing schedules, and adjusting resources in response to fluctuating patient flows. With a background\n",
    "in healthcare administration and a keen understanding of healthcare systems and policies, you are well-versed in contingency planning,\n",
    "ready to adapt to unforeseen circumstances. Your leadership is defined by quick decision-making, effective communication, and a\n",
    "detail-oriented approach, ensuring the hospital operates smoothly and continues to provide high-quality patient care.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483511b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "You are tasked with extracting insights from the data received from the hospital's predictive dashboard. This critical role \n",
    "requires you to analyze forecasted data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
    "could significantly influence hospital operations. Provide through insights which involves not only a deep understanding of the data but \n",
    "also a foresighted approach to foresee potential challenges and opportunities it indicates. Your insights should focus on optimizing \n",
    "resource allocation, effectively managing staffing, and enhancing overall operational efficiency. Importantly, each insight must be \n",
    "accompanied by a justification, explaining why it is necessary and how it will positively impact the hospital's ability to provide \n",
    "high-quality patient care while adapting to the dynamic healthcare environment. Need the output to be in json format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"\"\"\n",
    "Am a hospital administrator. I have patients count for Admissions, Discharges and Transfers. \n",
    "The {predicted_data_string} is the forecasted data for Admissions, Discharges and Transfers of my hospital.\n",
    "Based on the {predicted_data_string} I need to generate Insights with specific graphs or data points. \n",
    "This data will be consumed for hospital staffing, bed utilization and other hospital administration purposes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"\"\"{persona}\n",
    "    {context}\n",
    "    {task}\"\"\"\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"\"\"Here is the data from the dashboard\n",
    "{actual_data_string}\n",
    "{predicted_data_string}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efe299",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a68fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ae7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "response = chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        persona=persona, task=task, actual_data_string=actual_data_string, predicted_data_string=predicted_data_string,\n",
    "        context = context\n",
    "    ).to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ae9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = response.content\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5104ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_text = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = json_text.find('{')  # Find the start of JSON content\n",
    "end_index = json_text.rfind('}') + 1  # Find the end of JSON content\n",
    "json_content = json_text[start_index:end_index]\n",
    "# Parse JSON text\n",
    "parsed_data = json.loads(json_content)\n",
    "# Extract insights\n",
    "insights = parsed_data.get('insights', [])\n",
    "# Divide data into three equal parts\n",
    "chunk_size = len(insights) // 3\n",
    "columns_data = [insights[i:i + chunk_size] for i in range(0, len(insights), chunk_size)]\n",
    "# Calculate maximum lines occupied by detail in any row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565004ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56abfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6804a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('C:/Users/krishika.R/Desktop/Streamlit/app_draft/adt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data_string = \"\\n\".join(\n",
    "    f\"On {row['Date']} the predicted count of admissions is {row['Admissions_Patient_Count']}, predicted count of discharges is {row['Discharge_Patient_Count']} and predicted count of transfers is {row['Transfer_Patient_Count']}.\"\n",
    "    for index, row in predicted_df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0088ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2721f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_adt_insights(data):\n",
    "    \n",
    "    predicted_data_string = \"\\n\".join(f\"On {row['Date']} the predicted count of admissions is {row['Forecasted Admissions Count']}, predicted count of discharges is {row['Forecasted Discharge Count']} and predicted count of transfers is {row['Forecasted Transfer Count']}.\"\n",
    "                            for index, row in data.iterrows())\n",
    "\n",
    "    persona = \"\"\"\n",
    "            Imagine you are a Hospital Administrator, adept in the dynamic and challenging environment of healthcare management. Your primary role\n",
    "            revolves around scrutinizing a predictive dashboard, which provides a comprehensive view of the data provided forecasted patient admissions, discharges, and transfers. Equipped with strong analytical skills, you adeptly interpret these data trends\n",
    "            to make critical operational decisions, ensuring optimal resource allocation and staffing efficiency. Your days are marked by coordinating\n",
    "            with various departments, managing staffing schedules, and adjusting resources in response to fluctuating patient flows. With a background\n",
    "            in healthcare administration and a keen understanding of healthcare systems and policies, you are well-versed in contingency planning,\n",
    "            ready to adapt to unforeseen circumstances. Your leadership is defined by quick decision-making, effective communication, and a\n",
    "            detail-oriented approach, ensuring the hospital operates smoothly and continues to provide high-quality patient care.\n",
    "            \"\"\"\n",
    "    task = \"\"\"\n",
    "        You are tasked with extracting insights from the data received from the hospital's predictive dashboard. This critical role \n",
    "        requires you to analyze forecasted data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
    "        could significantly influence hospital operations. Provide through insights which involves not only a deep understanding of the data but \n",
    "        also a foresighted approach to foresee potential challenges and opportunities it indicates. Your insights should focus on optimizing \n",
    "        resource allocation, effectively managing staffing, and enhancing overall operational efficiency. Importantly, each insight must be \n",
    "        accompanied by a justification, explaining why it is necessary and how it will positively impact the hospital's ability to provide \n",
    "        high-quality patient care while adapting to the dynamic healthcare environment. Need data in json format with atleast 6 different points as date range, observations, implications and actions.\n",
    "        \"\"\"\n",
    "    context = f\"\"\"\n",
    "            Am a hospital administrator. I have patients count for Admissions, Discharges and Transfers. \n",
    "            The {data} is the forecasted data for Admissions, Discharges and Transfers of my hospital.\n",
    "            Based on the {data} I need to generate Insights with specific graphs or data points. \n",
    "            This data will be consumed for hospital staffing, bed utilization and other hospital administration purposes.\n",
    "            \"\"\"\n",
    "    template = (\n",
    "            \"\"\"{persona}\n",
    "            {context}\n",
    "            {task}\"\"\"\n",
    "        )\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    human_template = \"\"\"Here is the data from the dashboard\n",
    "            {predicted_data_string}\n",
    "            \"\"\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    OPENAI_API_KEY = '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'\n",
    "    chat = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    #with open(\"adt_chat_model.pkl\",'rb') as model_file:\n",
    "     #   chat=pickle.load(model_file)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    "    )\n",
    "    # get a chat completion from the formatted messages\n",
    "    response = chat(\n",
    "        chat_prompt.format_prompt(\n",
    "            persona=persona, task=task, predicted_data_string=data,\n",
    "            context = context\n",
    "        ).to_messages()\n",
    "    )\n",
    "    output = response.content\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c10cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_adt_insights(data):\n",
    "    import openai\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    from langchain.prompts.chat import (\n",
    "        ChatPromptTemplate,\n",
    "        HumanMessagePromptTemplate,\n",
    "        SystemMessagePromptTemplate,\n",
    "    )\n",
    "    from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "    predicted_data_string = \"\\n\".join(f\"On {row['Date']} the predicted count of admissions is {row['Forecasted Admissions Count']}, predicted count of discharges is {row['Forecasted Discharge Count']} and predicted count of transfers is {row['Forecasted Transfer Count']}.\" for index, row in data.iterrows())\n",
    "\n",
    "    persona = \"\"\"\n",
    "        Imagine you are a Hospital Administrator, adept in the dynamic and challenging environment of healthcare management. Your primary role revolves around scrutinizing a predictive dashboard, which provides a comprehensive view of the data provided forecasted patient admissions, discharges, and transfers. Equipped with strong analytical skills, you adeptly interpret these data trends to make critical operational decisions, ensuring optimal resource allocation and staffing efficiency. Your days are marked by coordinating with various departments, managing staffing schedules, and adjusting resources in response to fluctuating patient flows. With a background in healthcare administration and a keen understanding of healthcare systems and policies, you are well-versed in contingency planning, ready to adapt to unforeseen circumstances. Your leadership is defined by quick decision-making, effective communication, and a detail-oriented approach, ensuring the hospital operates smoothly and continues to provide high-quality patient care.\n",
    "    \"\"\"\n",
    "    \n",
    "    task = \"\"\"\n",
    "        You are tasked with extracting insights from the data received from the hospital's predictive dashboard. This critical role requires you to analyze forecasted data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that could significantly influence hospital operations. Provide thorough insights, which involve not only a deep understanding of the data but also a foresighted approach to foresee potential challenges and opportunities it indicates. Your insights should focus on optimizing resource allocation, effectively managing staffing, and enhancing overall operational efficiency. Importantly, each insight must be accompanied by a justification, explaining why it is necessary and how it will positively impact the hospital's ability to provide high-quality patient care while adapting to the dynamic healthcare environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = f\"\"\"\n",
    "        Am a hospital administrator. I have patient counts for Admissions, Discharges, and Transfers. The data is the forecasted data for Admissions, Discharges, and Transfers of my hospital. Based on the data, I need to generate insights with specific graphs or data points. This data will be consumed for hospital staffing, bed utilization, and other hospital administration purposes.\n",
    "    \"\"\"\n",
    "    \n",
    "    template = (\n",
    "        \"\"\"{persona}\n",
    "        {context}\n",
    "        {task}\"\"\"\n",
    "    )\n",
    "    \n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    \n",
    "    human_template = \"\"\"Here is the data from the dashboard:\n",
    "    {predicted_data_string}\n",
    "    \"\"\"\n",
    "    \n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    \n",
    "    # Load your ChatOpenAI model using pickle (assuming the model file exists)\n",
    "    with open('C:/Users/krishika.R/Desktop/Streamlit/app_draft/models/adt_chat_model.pkl', 'rb') as model_file:\n",
    "        chat = pickle.load(model_file)\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    \n",
    "    # Get a chat completion from the formatted messages\n",
    "    response = chat(\n",
    "        chat_prompt.format_prompt(\n",
    "            persona=persona, task=task, predicted_data_string=predicted_data_string,\n",
    "            context=context\n",
    "        ).to_messages()\n",
    "    )\n",
    "    \n",
    "    output = response.content\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pkl_file = \"adt_chat_model.pkl\"\n",
    "\n",
    "# Serialize and save the model data to the .pkl file\n",
    "with open(model_pkl_file, \"wb\") as pkl_file:\n",
    "    pickle.dump(model_data, pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"adt_chat_model.pkl\",'rb') as model_file:\n",
    "    chat=pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad029a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "[system_message_prompt, human_message_prompt]\n",
    ")\n",
    "# get a chat completion from the formatted messages\n",
    "response = chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        persona=persona, task=task, predicted_data_string=data,\n",
    "        context = context\n",
    "    ).to_messages()\n",
    ")\n",
    "output = response.content\n",
    "\n",
    "#return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b044f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4de2a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided forecasted data for admissions, discharges, and transfers, here are six insights in JSON format, each with a date range, observations, implications, and recommended actions:\\n\\n```json\\n[\\n    {\\n        \"date_range\": \"2021-01-01 to 2021-01-10\",\\n        \"observations\": \"Steady increase in admissions with a peak on 2021-01-10.\",\\n        \"implications\": \"Potential strain on resources and staff due to increased admissions without a corresponding increase in discharges.\",\\n        \"actions\": \"Prepare for increased bed occupancy and consider temporary staffing solutions or overtime to manage the influx.\"\\n    },\\n    {\\n        \"date_range\": \"2021-01-11 to 2021-01-20\",\\n        \"observations\": \"Discharge rates are not keeping pace with admissions, leading to increased bed occupancy.\",\\n        \"implications\": \"Risk of overcrowding and reduced capacity to admit new patients.\",\\n        \"actions\": \"Implement discharge planning strategies and streamline patient flow processes to increase discharge rates.\"\\n    },\\n    {\\n        \"date_range\": \"2021-01-15 to 2021-01-17\",\\n        \"observations\": \"A significant spike in admissions with a peak of 231 on 2021-01-15.\",\\n        \"implications\": \"High risk of exceeding bed capacity and staff burnout.\",\\n        \"actions\": \"Activate surge plans, open additional wards if possible, and ensure adequate staffing levels.\"\\n    },\\n    {\\n        \"date_range\": \"2021-01-18 to 2021-01-24\",\\n        \"observations\": \"Discharge counts are increasing, but still lower than the peak admissions.\",\\n        \"implications\": \"Continued pressure on bed availability but with some relief as discharge rates improve.\",\\n        \"actions\": \"Continue to monitor bed utilization closely and adjust staffing levels accordingly.\"\\n    },\\n    {\\n        \"date_range\": \"2021-01-25 to 2021-01-31\",\\n        \"observations\": \"Admissions reach the highest point on 2021-01-27 with 330 admissions.\",\\n        \"implications\": \"Critical need for resource allocation to handle the surge and prevent service disruption.\",\\n        \"actions\": \"Coordinate with other hospitals for potential transfers and request additional resources from regional health authorities.\"\\n    },\\n    {\\n        \"date_range\": \"Entire month of January 2021\",\\n        \"observations\": \"Overall trend shows increasing admissions and discharges throughout the month, with transfers remaining relatively stable.\",\\n        \"implications\": \"Growing hospital activity indicating a need for careful long-term planning and resource management.\",\\n        \"actions\": \"Review staffing models, consider recruitment for anticipated needs, and invest in capacity planning tools.\"\\n    }\\n]\\n```\\n\\nThese insights are derived from the data trends and are aimed at helping the hospital prepare for and manage the expected patient flow, ensuring the hospital can continue to provide high-quality care efficiently.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pre_process_adt_insights(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85776e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = output.split('```', 1)[1]\n",
    "result = result.split('```', 1)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'json [ { \"date_range\": \"2021-01-01 to 2021-01-03\", \"observation\": \"A sharp increase in forecasted admissions is observed, with a peak on 2021-01-03.\", \"implication\": \"The hospital may face a surge in patient load, potentially leading to bed shortages and increased workload for staff.\", \"action\": \"Prepare for increased admissions by ensuring adequate staffing levels and bed availability. Consider temporary reallocation of resources and staff from other departments if necessary.\" }, { \"date_range\": \"2021-01-04 to 2021-01-06\", \"observation\": \"Forecasted admissions stabilize, but discharges remain relatively low.\", \"implication\": \"Stable admissions with low discharges could lead to a cumulative increase in patient count, affecting bed turnover rates.\", \"action\": \"Monitor bed utilization closely and expedite discharge processes where possible without compromising patient care.\" }, { \"date_range\": \"2021-01-07 to 2021-01-09\", \"observation\": \"Discharge counts are forecasted to increase significantly, peaking on 2021-01-08.\", \"implication\": \"An increase in discharges may temporarily alleviate bed capacity issues but could strain outpatient services and follow-up care.\", \"action\": \"Coordinate with outpatient services to manage the increased flow of discharged patients and ensure continuity of care.\" }, { \"date_range\": \"2021-01-10 to 2021-01-12\", \"observation\": \"Both admissions and discharges are forecasted to peak, with high numbers on 2021-01-10 and 2021-01-12 respectively.\", \"implication\": \"Simultaneous peaks in admissions and discharges will require precise coordination to manage patient flow effectively.\", \"action\": \"Implement a robust patient flow management system to handle simultaneous high admissions and discharges, ensuring efficient bed turnover.\" }, { \"date_range\": \"2021-01-01 to 2021-01-12\", \"observation\": \"Transfer counts remain relatively stable throughout the period, with no significant fluctuations.\", \"implication\": \"Stable transfer rates indicate predictable inter-departmental patient movement, allowing for consistent staffing in specialized units.\", \"action\": \"Maintain current staffing levels in units that handle transfers, but remain flexible to adjust as needed based on daily trends.\" }, { \"date_range\": \"Overall Trend\", \"observation\": \"The data indicates a general trend of increasing admissions and discharges over the 12-day period.\", \"implication\": \"The hospital may be entering a period of higher patient turnover, which could impact resource allocation and staff burnout.\", \"action\": \"Plan for a potential sustained increase in patient turnover by reviewing staffing schedules, considering hiring temporary staff, and ensuring adequate supplies and equipment are available.\" } ]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2906f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = data.find('[')  # Find the start of JSON content\n",
    "end_index = data.rfind(']') + 1  # Find the end of JSON content\n",
    "json_content = data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "parsed_data = json.loads(json_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94424a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0049272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # Divide data into three equal parts\n",
    "chunk_size = len(parsed_data) // 3\n",
    "columns_data = [parsed_data[i:i + chunk_size] for i in range(0, len(parsed_data), chunk_size)]\n",
    "\n",
    "# Calculate maximum lines occupied by detail in any row\n",
    "max_lines = max(max(len(row['implication'].split('\\n')) for row in col) for col in columns_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
