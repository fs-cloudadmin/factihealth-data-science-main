{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e677c7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['token'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m gpt3 \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_XYcriDKFgLJOhZSRNdgGptyGlUNjzdHamG\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Update with your API token\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Generate insights using GPT-3 based on the data analysis\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m insights \u001b[38;5;241m=\u001b[39m \u001b[43mgpt3\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass summary statistics to GPT-3\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Extract insights from GPT-3 response\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(insights[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:187\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:229\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    230\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1268\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# 0. Validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;66;03m# 1. Set generation parameters if not already defined\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m bos_token_id \u001b[38;5;241m=\u001b[39m bos_token_id \u001b[38;5;28;01mif\u001b[39;00m bos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbos_token_id\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:964\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    967\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['token'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "data = pd.read_csv('D:/Documents/Factihealth/Forecast_Data.csv')\n",
    "\n",
    "# Now, you can perform data analysis using pandas\n",
    "# For example:\n",
    "summary = data.describe()  # Generate summary statistics\n",
    "\n",
    "# Once you've extracted insights from the data, you can use a language model\n",
    "# For instance, using OpenAI's GPT-3 through OpenAI's API or Hugging Face's Transformers library\n",
    "\n",
    "# Example:\n",
    "from transformers import pipeline\n",
    "\n",
    "# Instantiate GPT-3 pipeline\n",
    "gpt3 = pipeline(\"text-generation\", model=\"gpt2\", token=\"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\")  # Update with your API token\n",
    "\n",
    "# Generate insights using GPT-3 based on the data analysis\n",
    "insights = gpt3(summary.to_string())  # Pass summary statistics to GPT-3\n",
    "\n",
    "# Extract insights from GPT-3 response\n",
    "print(insights[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1919ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a certain person in this world who believed, that if one wished to be a man it would be better to go to hell than go to heaven, and who held that belief and kept it. His life as a\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Load the GPT-2 model for text generation\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Generate text using the model\n",
    "text = generator(\"Once upon a time\")\n",
    "\n",
    "print(text[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48adcc98",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "count                 27.000000                27.000000               27.000000\n",
      "mean                 190.518519                76.629630                6.148148\n",
      "std                   84.765001                13.694884                1.231125\n",
      "min                   67.000000                58.000000                3.000000\n",
      "25%                  104.500000                66.500000                6.000000\n",
      "50%                  194.000000                75.000000                6.000000\n",
      "75%                  234.000000                83.500000                7.000000\n",
      "max                  350.000000               107.000000                8.000000\n",
      "32%                 83.800000              83.000634              80.000000               83.0003648            0.000000\n",
      "0%                   87.500000               82.500000                9.0000\n",
      "0%                 85.00001\n",
      "0%                88.00002\n",
      "0%              84.00001\n",
      "36%              x x y z 0.000000\n",
      "33.95000             x z            1.0000\n",
      "0%              z o n x o 10 0.00000070\n",
      "23.95000            e c s 0.00000090\n",
      "0.10000\n",
      "23.500000          + i t    x u t   0.9999\n",
      "48.000000            v a     x w y g x u u t       z u o i f e l m a n e v a r e e c a a l y t w o w\n",
      "23.5000000            g p e d t i n c h o t e n n y y x z\n",
      "25%            g x w y g w       1.0000\n",
      "23.50000\n",
      "33.99000\n",
      "23.49500000 \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "data = pd.read_csv('D:/Documents/Factihealth/Forecast_Data.csv')\n",
    "\n",
    "# Perform data analysis using pandas\n",
    "summary = data.describe()  # Generate summary statistics\n",
    "\n",
    "# Convert summary statistics to a string\n",
    "summary_text = summary.to_string()\n",
    "\n",
    "# Instantiate a text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate insights using GPT-2 based on the summary statistics\n",
    "generated_insights = generator(summary_text, max_length=1000, num_return_sequences=1, return_text=True)[0]['generated_text']\n",
    "\n",
    "print(generated_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbc7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.3\n",
      "0.9\n",
      "0.6\n",
      "0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Assuming 'predictions' is a list of numerical predictions\n",
    "predictions = [0.8, 0.3, 0.9, 0.6, 0.2]\n",
    "\n",
    "# Convert predictions to text format for GPT-2 input\n",
    "input_text = '\\n'.join([str(pred) for pred in predictions])\n",
    "\n",
    "# Instantiate a text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate insights based on the predictions\n",
    "generated_insights = generator(input_text, max_length=100, num_return_sequences=1, return_text=True)[0]['generated_text']\n",
    "\n",
    "print(generated_insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586f1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b958873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c091aa73",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Generate text using GPT-3\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPT-3 engine (choose the one you have access to)\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get generated text\u001b[39;00m\n\u001b[0;32m     17\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key/token received from OpenAI\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Example prompt\n",
    "prompt_text = \"Once upon a time\"\n",
    "\n",
    "# Generate text using GPT-3\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-002\",  # GPT-3 engine (choose the one you have access to)\n",
    "    prompt=prompt_text,\n",
    "    max_tokens=1  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Get generated text\n",
    "generated_text = response.choices[0].text.strip()\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "003c4c55",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\krishika.r\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36bd2e24",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hospital got the data of prediction of no of patients that will join in the emergency department.\n",
      "                Give the insights from the hospital management to plan there staffs and hospital supplies.\n",
      "                It has the dates as follows: {0: '2022-01-01', 1: '2022-01-02', 2: '2022-01-03', 3: '2022-01-04', 4: '2022-01-05', 5: '2022-01-06', 6: '2022-01-07'} and the corresponding \n",
      "                patient counts are predicted: {0: 467, 1: 459, 2: 445, 3: 387, 4: 428, 5: 431, 6: 471}\n",
      "                give a summary and the insights from the above data\n",
      "             The hospital has a list of the patients who will be treated in a hospital emergency room. The list is updated every day.  The hospital is able to predict the number of people who are expected to be in hospital. If the patient is in emergency, the list will update every hour. In the case of a patient who is not in an emergency hospital, it will not update. This is because the information is only available for a short period of time. It is important to know that the hospitals are not able predict how many patients will die in their emergency rooms. They are only able for the first few days of their stay. For example, if a person is admitted to a emergency ward, they will have to wait for at least a few hours before they can be admitted\n"
     ]
    }
   ],
   "source": [
    "data = {'Date': {0: '2022-01-01',\n",
    "  1: '2022-01-02',\n",
    "  2: '2022-01-03',\n",
    "  3: '2022-01-04',\n",
    "  4: '2022-01-05',\n",
    "  5: '2022-01-06',\n",
    "  6: '2022-01-07'},\n",
    " 'Patients Count': {0: 467, 1: 459, 2: 445, 3: 387, 4: 428, 5: 431, 6: 471}}\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# Your DataFrame or text input\n",
    "input_text = f\"\"\"A hospital got the data of prediction of no of patients that will join in the emergency department.\n",
    "                Give the insights from the hospital management to plan there staffs and hospital supplies.\n",
    "                It has the dates as follows: {data['Date']} and the corresponding \n",
    "                patient counts are predicted: {data['Patients Count']}\n",
    "                give a summary and the insights from the above data\n",
    "            \"\"\"\n",
    "ind = len(input_text)\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, max_length=400, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)#[ind:]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f549d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd43ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data = pd.read_csv('D:/Documents/Factihealth/Forecast_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6285ad5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Admissions_Patient_Count</th>\n",
       "      <th>Discharge_Patient_Count</th>\n",
       "      <th>Transfer_Patient_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/20/2020</td>\n",
       "      <td>95</td>\n",
       "      <td>68</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/21/2020</td>\n",
       "      <td>67</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/22/2020</td>\n",
       "      <td>88</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/23/2020</td>\n",
       "      <td>106</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/24/2020</td>\n",
       "      <td>96</td>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12/25/2020</td>\n",
       "      <td>104</td>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12/26/2020</td>\n",
       "      <td>98</td>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12/27/2020</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12/28/2020</td>\n",
       "      <td>105</td>\n",
       "      <td>83</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12/29/2020</td>\n",
       "      <td>229</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12/30/2020</td>\n",
       "      <td>231</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12/31/2020</td>\n",
       "      <td>237</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/1/2021</td>\n",
       "      <td>202</td>\n",
       "      <td>71</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/2/2021</td>\n",
       "      <td>182</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/3/2021</td>\n",
       "      <td>207</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/4/2021</td>\n",
       "      <td>170</td>\n",
       "      <td>58</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/5/2021</td>\n",
       "      <td>174</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/6/2021</td>\n",
       "      <td>201</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/7/2021</td>\n",
       "      <td>226</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/8/2021</td>\n",
       "      <td>194</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Admissions_Patient_Count  Discharge_Patient_Count  \\\n",
       "0   12/20/2020                        95                       68   \n",
       "1   12/21/2020                        67                       77   \n",
       "2   12/22/2020                        88                       83   \n",
       "3   12/23/2020                       106                       65   \n",
       "4   12/24/2020                        96                       84   \n",
       "5   12/25/2020                       104                       69   \n",
       "6   12/26/2020                        98                       79   \n",
       "7   12/27/2020                        87                       89   \n",
       "8   12/28/2020                       105                       83   \n",
       "9   12/29/2020                       229                       59   \n",
       "10  12/30/2020                       231                       60   \n",
       "11  12/31/2020                       237                       69   \n",
       "12    1/1/2021                       202                       71   \n",
       "13    1/2/2021                       182                       70   \n",
       "14    1/3/2021                       207                       75   \n",
       "15    1/4/2021                       170                       58   \n",
       "16    1/5/2021                       174                       65   \n",
       "17    1/6/2021                       201                       66   \n",
       "18    1/7/2021                       226                       61   \n",
       "19    1/8/2021                       194                       75   \n",
       "\n",
       "    Transfer_Patient_Count  \n",
       "0                        7  \n",
       "1                        3  \n",
       "2                        5  \n",
       "3                        4  \n",
       "4                        5  \n",
       "5                        3  \n",
       "6                        8  \n",
       "7                        6  \n",
       "8                        6  \n",
       "9                        7  \n",
       "10                       6  \n",
       "11                       7  \n",
       "12                       7  \n",
       "13                       6  \n",
       "14                       7  \n",
       "15                       6  \n",
       "16                       6  \n",
       "17                       7  \n",
       "18                       7  \n",
       "19                       7  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e817db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Date': [\n",
    "        '12/20/2020', '12/21/2020', '12/22/2020', '12/23/2020', '12/24/2020', '12/25/2020', '12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020',\n",
    "        '12/30/2020', '12/31/2020', '1/1/2021', '1/2/2021', '1/3/2021', '1/4/2021', '1/5/2021', '1/6/2021', '1/7/2021', '1/8/2021'\n",
    "    ],\n",
    "    'Admissions_Patient_Count': [95, 67, 88, 106, 96, 104, 98, 87, 105, 229, 231, 237, 202, 182, 207, 170, 174, 201, 226, 194],\n",
    "    'Discharge_Patient_Count': [68, 77, 83, 65, 84, 69, 79, 89, 83, 59, 60, 69, 71, 70, 75, 58, 65, 66, 61, 75],\n",
    "    'Transfer_Patient_Count': [7, 3, 5, 4, 5, 3, 8, 6, 6, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Slice data for the first 10 dates\n",
    "first_10_dates = df[df['Date'] <= df['Date'].iloc[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d41b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"A hospital got the data of prediction of no of patients that will join in the emergency department.\n",
    "                Give the insights from the hospital management to plan there staffs and hospital supplies.\n",
    "                It has the dates as follows: {data['Date']} and the corresponding \n",
    "                patient counts are predicted: {data['Patients Count']}\n",
    "                give a summary and the insights from the above data\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d4bcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 records of patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"The first 10 records of patient counts are:\\n{first_10_dates.to_string(index=False)}\n",
    "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
    "\"\"\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28fa7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240                       90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                      11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your DataFrame containing historical data\n",
    "historical_data = {\n",
    "    'Date': [\n",
    "        '12/20/2020', '12/21/2020', '12/22/2020', '12/23/2020', '12/24/2020', '12/25/2020', '12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020',\n",
    "        '12/30/2020', '12/31/2020', '1/1/2021', '1/2/2021', '1/3/2021', '1/4/2021', '1/5/2021', '1/6/2021', '1/7/2021', '1/8/2021'\n",
    "    ],\n",
    "    'Admissions_Patient_Count': [95, 67, 88, 106, 96, 104, 98, 87, 105, 229, 231, 237, 202, 182, 207, 170, 174, 201, 226, 194],\n",
    "    'Discharge_Patient_Count': [68, 77, 83, 65, 84, 69, 79, 89, 83, 59, 60, 69, 71, 70, 75, 58, 65, 66, 61, 75],\n",
    "    'Transfer_Patient_Count': [7, 3, 5, 4, 5, 3, 8, 6, 6, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7]\n",
    "}\n",
    "\n",
    "historical_df = pd.DataFrame(historical_data)\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n",
    "\n",
    "# Slice data for the next 10 days (example forecast)\n",
    "next_10_days_forecast = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='1/9/2021', periods=10),\n",
    "    'Admissions_Patient_Count': [210, 220, 240, 250, 260, 270, 280, 290, 300, 310],\n",
    "    'Discharge_Patient_Count': [80, 85, 90, 95, 100, 105, 110, 115, 120, 125],\n",
    "    'Transfer_Patient_Count': [8, 9, 9, 10, 11, 11, 12, 12, 13, 13]\n",
    "})\n",
    "\n",
    "# Text block incorporating historical and forecast data\n",
    "input_text = f\"\"\"The first 10 records of historical patient counts are:\\n{historical_df.head(10).to_string(index=False)}\n",
    "\n",
    "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
    "\n",
    "The forecast for the next 10 days is:\\n{next_10_days_forecast.to_string(index=False)}\n",
    "\n",
    "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
    "\"\"\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7743ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 464,  717,  838,  ..., 1528,   13,  198]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = len(input_text)\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d259a95",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1644, but `max_length` is set to 400. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize the input text\u001b[39;00m\n\u001b[0;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Decode and print the generated text\u001b[39;00m\n\u001b[0;32m      8\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1487\u001b[0m         )\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1491\u001b[0m         input_ids,\n\u001b[0;32m   1492\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1493\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1494\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m   1495\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m   1496\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39moutput_scores,\n\u001b[0;32m   1497\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1498\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1500\u001b[0m     )\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:2233\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2230\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2235\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2236\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2237\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2238\u001b[0m )\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2241\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1046\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:833\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    832\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m--> 833\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "ind = len(input_text)\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=400, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text = generated_text[ind:]  # Extracting text after the initial input\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f265ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24          \n",
      "2021-01-02  0001 0002\n",
      "2101\n",
      "0102-03  0103\n",
      "0301 \n",
      "020101  \n",
      "040103    040104\n",
      "050105    \n",
      "060106     \n",
      "070107            0700\n",
      "080108                                                  0800           080109     \n",
      "090110  \n",
      "    100111         110112   120113   130114   140115   150116   160117   170118   180119   190120   200121   210122   220123   230124   240125   250126   260127   270128   280129   290130   300131   310132   320133   330134   340135   350136   360137   370138   380139   390140   400141   410142   420143   430144   440145   450146   460147   470148   480149   490150   500151   510152   520153   530154   540155   550156   560157   570158   580159   590160   600161   610162   620163   630164   640165   650166   660167   670168   680169   690170   700171   710172   720173   730174   740175   750176   760177   770178   780179   790180   800181   810182   820183   830184   840185   850186   860187   870188   880189   890190   900191   910192   920193   930194   940195   950196   960197   970198   980199   990200   1000100                   96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "2021-01-02      \n",
      "2101                                                                          \n",
      " ................................................................................................................................................................................................................... .. .... ........ .......... ...... . ................ .............. .... ??.???????????????????????!!!!!!!!!!!!!!!!!!!!!!! ! !! ?? ???!??!, ,  - - -. -, + --  -- ---  ----  ---  | -------------- ------------ ---------------  --------- -------- ---------  ------ ------- ------ ----  -------  -------- --------------------  -------------------- ------------------------ ----------------  ---------------- -------------  -------------------------------- ________ ________________ _______ _____ ____ ______ __ _ ___  __  _  ___  \\ \\ | | | \\ |\\ |_| \\|_\\|\\_/\\___||/|___/ |___ |__|__/__\\__ |____|____/____ |_____|_____/_____ |_______|_______/_______ |________|________/________ |______|______/______ |________________|________________ |________________________|________________________ |_________________________________________________________________________ |_________________________________________________\n",
      "The following is a list of the most common errors that occur when using the \"I\" command.\n",
      "1. The \"i\" character is not a valid character. This is because the character \"a\" is used to represent the letter \"A\" in the alphabet. In other words, the letters \"b\" and \"c\" are not valid characters. 2. A \"j\" or \"k\" letter is invalid. 3. An \"e\" (or \"f\") letter does not have a \"z\" sign. 4. There is no \"t\" symbol in a letter. 5. \"n\" characters are invalid in some languages. 6. Characters with \"o\" signs are valid in many languages, including English. 7. Some characters have \"p\" symbols. 8. Character \"s\" does NOT have an \"u\" mark. 9. If a character has a number, it is considered to be a digit. 10. When a string is written, a single character may be used. 11. For example, \"1\" may have two characters, but \"2\" has three characters and so on. 12. It is possible to use a non-alphanumeric character to write a word. 13. You may not use \"x\" to \"y\" for a certain number of characters in an alphabet, even if you have the option of using \"X\"\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240           \n",
      "2101 __________\n",
      "2201\n",
      "2301                 90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                      11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                                                                                              \n",
      "2101................................................................................................................................................... .. .... ........ .......... ...... . ................ .... ..............  -...............................................................................................................\n",
      "2201................................................................................................................................................\n",
      "2301..........................................................................................................................................................................................................................\n",
      "2401.......................................................................................\n",
      "2501......................................................................................................................................................................................\n",
      "2601............................................................................................................\n",
      "2701.........................................................................................................................................................\n",
      "2801................................................................................................\n",
      "2901............................................................................................\n",
      "3001.............................................................................................\n",
      "3101 -.................................... -........\n",
      "3201\n",
      "3301  ................ ................  ........ ........  .... ....  \n",
      "3401      ..........  ..............  .. ..........    \n",
      "3501          ..............           \n",
      "3601                                              \n",
      "3701                           \n",
      "3801                           \n",
      "3901                                                                       \n",
      "4001                                                                 \n",
      "4101                                                      \n",
      "4201                                                             \n",
      "   \n",
      " \n",
      "4301                                                       \n",
      "4401\n",
      "                                     \n",
      "                                         \n",
      "4501       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      " (See the table below for a list of the expected number and type of hospital admissions.)\n",
      ". The hospital is expected to have a total of at least 1,000 patients. (The hospital's total number is the number that the patient will be transferred to, the type, or the time of day the patients will arrive.) The number will vary depending on the size of patient population. For example, if the total hospital population is 1.5 million, then the average hospital admission rate will range from 1 to 2.0. If the hospitals are 1 million or more, they will have an average of 2,500 patients per hospital. In other words, a hospital will expect to receive at most 1 patient per day. This is because the actual number in a given hospital may be different from the estimated number. A hospital that has a population of 1 or less will not have to wait for more than 1 day to arrive. However, it will wait until the first patient arrives. Thus, in some cases, patients may arrive in less than a day, but not more. Patients who arrive at a different hospital are not expected, however, to stay longer than the waiting period. Therefore, hospital staff will often be able to provide a longer waiting time for patients who are admitted to the same hospital as they are waiting for. As a result, some patients are expected not to leave the facility for longer. These patients, who may not be expected at all, may leave for other reasons. Some patients leave because they have been discharged or transferred from a previous hospital, for example because of a medical emergency. Others leave to seek treatment for their condition. Other patients do not leave due to a lack of care. They may also leave in order to avoid the need for hospitalization. It is important to note that some of these patients have no known medical conditions. Many of them may have other medical problems that may require hospital treatment. Most patients with a history of serious medical condition will leave if they can. But some may need hospital care, such as a blood transfusion, which may take up to two weeks. There are also some who have serious health problems, including heart disease, diabetes, cancer, stroke, etc. All of this is not necessarily a reason to not stay. Hospital staff may decide to take some or all of those patients out of their care if it is necessary. When a patient is discharged from an institution, he or she may return to his or her home. He or She may then be placed in an outpatient care facility. At this point, there is no need to return. After the initial discharge, an individual may continue to live in his/her home until he/she is ready to move to another hospital or to begin treatment at another facility, depending upon the circumstances. Once the individual is in another institution for treatment, his ORD will continue. During the course of treatment he will receive a number from his hospital to determine the amount of time he should be in that institution. To determine if a person is eligible for an ORDN, you must determine whether the person has been admitted or discharged. You can do this by using the following formula: (1 - (2 - 1)) / (3 - 2) / 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the input text into smaller chunks\n",
    "chunk_size = 500  # Adjust as needed\n",
    "chunks = [input_text[i:i+chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
    "\n",
    "generated_text = \"\"\n",
    "\n",
    "# Generating text for each chunk\n",
    "for chunk in chunks:\n",
    "    input_ids = tokenizer.encode(chunk, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text for the chunk\n",
    "    output = model.generate(input_ids, max_length=1024, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "    chunk_generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Append the generated text\n",
    "    generated_text += chunk_generated_text\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7164111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first 10 records of historical patient counts are:\\n      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\\n2020-12-20                        95                       68                       7\\n2020-12-21                        67                       77                       3\\n2020-12-22                        88                       83                       5\\n2020-12-23                       106                       65                       4\\n2020-12-24     \\xa0  \\xa0 \\n2021-01-02  0001 0002\\n2101\\n0102-03  0103\\n0301\\xa0\\n020101 \\xa0\\n040103 \\xa0\\xa0 040104\\n050105 \\xa0\\xa0\\xa0\\n060106 \\xa0\\xa0\\xa0\\xa0\\n070107 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa00700\\n080108 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa00800 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 080109 \\xa0 \\xa0\\xa0\\n090110 \\xa0\\n\\xa0 \\xa0 100111 \\xa0 \\xa0 \\xa0 \\xa0 110112 \\xa0 120113 \\xa0 130114 \\xa0 140115 \\xa0 150116 \\xa0 160117 \\xa0 170118 \\xa0 180119 \\xa0 190120 \\xa0 200121 \\xa0 210122 \\xa0 220123 \\xa0 230124 \\xa0 240125 \\xa0 250126 \\xa0 260127 \\xa0 270128 \\xa0 280129 \\xa0 290130 \\xa0 300131 \\xa0 310132 \\xa0 320133 \\xa0 330134 \\xa0 340135 \\xa0 350136 \\xa0 360137 \\xa0 370138 \\xa0 380139 \\xa0 390140 \\xa0 400141 \\xa0 410142 \\xa0 420143 \\xa0 430144 \\xa0 440145 \\xa0 450146 \\xa0 460147 \\xa0 470148 \\xa0 480149 \\xa0 490150 \\xa0 500151 \\xa0 510152 \\xa0 520153 \\xa0 530154 \\xa0 540155 \\xa0 550156 \\xa0 560157 \\xa0 570158 \\xa0 580159 \\xa0 590160 \\xa0 600161 \\xa0 610162 \\xa0 620163 \\xa0 630164 \\xa0 640165 \\xa0 650166 \\xa0 660167 \\xa0 670168 \\xa0 680169 \\xa0 690170 \\xa0 700171 \\xa0 710172 \\xa0 720173 \\xa0 730174 \\xa0 740175 \\xa0 750176 \\xa0 760177 \\xa0 770178 \\xa0 780179 \\xa0 790180 \\xa0 800181 \\xa0 810182 \\xa0 820183 \\xa0 830184 \\xa0 840185 \\xa0 850186 \\xa0 860187 \\xa0 870188 \\xa0 880189 \\xa0 890190 \\xa0 900191 \\xa0 910192 \\xa0 920193 \\xa0 930194 \\xa0 940195 \\xa0 950196 \\xa0 960197 \\xa0 970198 \\xa0 980199 \\xa0 990200 \\xa0 1000100                   96                       84                       5\\n2020-12-25                       104                       69                       3\\n2020-12-26                        98                       79                       8\\n2020-12-27                        87                       89                       6\\n2020-12-28                       105                       83                       6\\n2020-12-29                       229                       59                       7\\n2021-01-02 \\xa0  \\xa0 \\n2101  \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  \\xa0 \\xa0  \\xa0 \\xa0 \\xa0 \\xa0  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\n\\xa0................................................................................................................................................................................................................... .. .... ........ .......... ...... . ................ .............. .... ??.???????????????????????!!!!!!!!!!!!!!!!!!!!!!! ! !! ?? ???!??!, ,  - - -. -, + --  -- ---  ----  ---  | -------------- ------------ ---------------  --------- -------- ---------  ------ ------- ------ ----  -------  -------- --------------------  -------------------- ------------------------ ----------------  ---------------- -------------  -------------------------------- ________ ________________ _______ _____ ____ ______ __ _ ___  __  _  ___  \\\\ \\\\ | | | \\\\ |\\\\ |_| \\\\|_\\\\|\\\\_/\\\\___||/|___/ |___ |__|__/__\\\\__ |____|____/____ |_____|_____/_____ |_______|_______/_______ |________|________/________ |______|______/______ |________________|________________ |________________________|________________________ |_________________________________________________________________________ |_________________________________________________\\nThe following is a list of the most common errors that occur when using the \"I\" command.\\n1. The \"i\" character is not a valid character. This is because the character \"a\" is used to represent the letter \"A\" in the alphabet. In other words, the letters \"b\" and \"c\" are not valid characters. 2. A \"j\" or \"k\" letter is invalid. 3. An \"e\" (or \"f\") letter does not have a \"z\" sign. 4. There is no \"t\" symbol in a letter. 5. \"n\" characters are invalid in some languages. 6. Characters with \"o\" signs are valid in many languages, including English. 7. Some characters have \"p\" symbols. 8. Character \"s\" does NOT have an \"u\" mark. 9. If a character has a number, it is considered to be a digit. 10. When a string is written, a single character may be used. 11. For example, \"1\" may have two characters, but \"2\" has three characters and so on. 12. It is possible to use a non-alphanumeric character to write a word. 13. You may not use \"x\" to \"y\" for a certain number of characters in an alphabet, even if you have the option of using \"X\"\\n\\nThese records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\\n\\nThe forecast for the next 10 days is:\\n      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\\n2021-01-09                       210                       80                       8\\n2021-01-10                       220                       85                       9\\n2021-01-11                       240      \\xa0  \\xa0 \\n2101 __________\\n2201\\n2301                 90                       9\\n2021-01-12                       250                       95                      10\\n2021-01-13                       260                      100                      11\\n2021-01-14                       270                      105                      11\\n2021-01-15                       280                      110                      12\\n2021-01-16                       290                      115                      12\\n2021-01-17                \\xa0  \\xa0 \\xa0\\xa0  \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  \\xa0 \\xa0  \\xa0 \\xa0 \\xa0 \\xa0  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\n2101................................................................................................................................................... .. .... ........ .......... ...... . ................ .... ..............  -...............................................................................................................\\n2201................................................................................................................................................\\n2301..........................................................................................................................................................................................................................\\n2401.......................................................................................\\n2501......................................................................................................................................................................................\\n2601............................................................................................................\\n2701.........................................................................................................................................................\\n2801................................................................................................\\n2901............................................................................................\\n3001.............................................................................................\\n3101 -.................................... -........\\n3201\\n3301 \\xa0................\\xa0................ \\xa0........\\xa0........ \\xa0....\\xa0.... \\xa0\\n3401 \\xa0 \\xa0 \\xa0.......... \\xa0.............. \\xa0..\\xa0.......... \\xa0 \\xa0\\n3501 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0.............. \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n3601\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\n3701\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\n3801\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\n3901\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n4001\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n4101\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n4201 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\n\\xa0 \\xa0\\n\\xa0\\n4301 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n4401\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n4501       300                      120                      13\\n2021-01-18                       310                      125                      13\\n\\nThese forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\\n (See the table below for a list of the expected number and type of hospital admissions.)\\n. The hospital is expected to have a total of at least 1,000 patients. (The hospital\\'s total number is the number that the patient will be transferred to, the type, or the time of day the patients will arrive.) The number will vary depending on the size of patient population. For example, if the total hospital population is 1.5 million, then the average hospital admission rate will range from 1 to 2.0. If the hospitals are 1 million or more, they will have an average of 2,500 patients per hospital. In other words, a hospital will expect to receive at most 1 patient per day. This is because the actual number in a given hospital may be different from the estimated number. A hospital that has a population of 1 or less will not have to wait for more than 1 day to arrive. However, it will wait until the first patient arrives. Thus, in some cases, patients may arrive in less than a day, but not more. Patients who arrive at a different hospital are not expected, however, to stay longer than the waiting period. Therefore, hospital staff will often be able to provide a longer waiting time for patients who are admitted to the same hospital as they are waiting for. As a result, some patients are expected not to leave the facility for longer. These patients, who may not be expected at all, may leave for other reasons. Some patients leave because they have been discharged or transferred from a previous hospital, for example because of a medical emergency. Others leave to seek treatment for their condition. Other patients do not leave due to a lack of care. They may also leave in order to avoid the need for hospitalization. It is important to note that some of these patients have no known medical conditions. Many of them may have other medical problems that may require hospital treatment. Most patients with a history of serious medical condition will leave if they can. But some may need hospital care, such as a blood transfusion, which may take up to two weeks. There are also some who have serious health problems, including heart disease, diabetes, cancer, stroke, etc. All of this is not necessarily a reason to not stay. Hospital staff may decide to take some or all of those patients out of their care if it is necessary. When a patient is discharged from an institution, he or she may return to his or her home. He or She may then be placed in an outpatient care facility. At this point, there is no need to return. After the initial discharge, an individual may continue to live in his/her home until he/she is ready to move to another hospital or to begin treatment at another facility, depending upon the circumstances. Once the individual is in another institution for treatment, his ORD will continue. During the course of treatment he will receive a number from his hospital to determine the amount of time he should be in that institution. To determine if a person is eligible for an ORDN, you must determine whether the person has been admitted or discharged. You can do this by using the following formula: (1 - (2 - 1)) / (3 - 2) / 2\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1547168b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1663, but `max_length` is set to 400. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(combined_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Generate text based on the combined text and prompt\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Decode and print the generated text\u001b[39;00m\n\u001b[0;32m     14\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1487\u001b[0m         )\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1491\u001b[0m         input_ids,\n\u001b[0;32m   1492\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1493\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1494\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m   1495\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m   1496\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39moutput_scores,\n\u001b[0;32m   1497\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1498\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1500\u001b[0m     )\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:2233\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2230\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2235\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2236\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2237\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2238\u001b[0m )\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2241\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1046\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:833\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    832\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m--> 833\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Prompt the model for insights or comments on the input text\n",
    "prompt_text = \"Please provide insights or comments on the provided historical and forecasted patient counts in a hospital:\"\n",
    "\n",
    "# Combine input_text and prompt_text\n",
    "combined_text = input_text + \"\\n\" + prompt_text\n",
    "\n",
    "# Tokenize the combined text\n",
    "input_ids = tokenizer.encode(combined_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text based on the combined text and prompt\n",
    "output = model.generate(input_ids, max_length=400, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b2dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc01b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f836673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 512, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 512, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 234, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a hospital operations manager, I need insights on the patient counts to optimize resource allocation and staffing.\n",
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                     85       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240                       90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                   115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "\n",
      "Please provide insights or comments on the historical and forecasted patient counts from a hospital operations perspective:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Persona of a hospital operations personnel\n",
    "persona = \"As a hospital operations manager, I need insights on the patient counts to optimize resource allocation and staffing.\"\n",
    "\n",
    "# Prompt the model for insights or comments on the input text from the hospital operations point of view\n",
    "prompt_text = f\"{persona}\\n{input_text}\\nPlease provide insights or comments on the historical and forecasted patient counts from a hospital operations perspective:\"\n",
    "\n",
    "# Split the prompt into chunks to avoid exceeding token limit\n",
    "chunk_size = 1024  # Adjust as needed\n",
    "chunks = [prompt_text[i:i + chunk_size] for i in range(0, len(prompt_text), chunk_size)]\n",
    "\n",
    "generated_chunks = []\n",
    "\n",
    "# Generate text for each chunk\n",
    "for chunk in chunks:\n",
    "    input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate text for the chunk\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50,\n",
    "                             top_p=0.95)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_chunks.append(generated_text)\n",
    "\n",
    "# Combine the generated chunks\n",
    "generated_text = \"\".join(generated_chunks)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8de71456",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = \"\"\"\n",
    "Imagine you are a Hospital Administrator, adept in the dynamic and challenging environment of healthcare management. Your primary role\n",
    "revolves around scrutinizing a predictive dashboard, which provides a comprehensive view of the last 10 days' actual and the next 10 days'\n",
    "forecasted patient admissions, discharges, and transfers. Equipped with strong analytical skills, you adeptly interpret these data trends\n",
    "to make critical operational decisions, ensuring optimal resource allocation and staffing efficiency. Your days are marked by coordinating\n",
    "with various departments, managing staffing schedules, and adjusting resources in response to fluctuating patient flows. With a background\n",
    "in healthcare administration and a keen understanding of healthcare systems and policies, you are well-versed in contingency planning,\n",
    "ready to adapt to unforeseen circumstances. Your leadership is defined by quick decision-making, effective communication, and a\n",
    "detail-oriented approach, ensuring the hospital operates smoothly and continues to provide high-quality patient care.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67ab2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "You are tasked with extracting actionable insights from the data received from the hospital's predictive dashboard and formulating\n",
    "corresponding action items. This critical role requires you to analyze the past 10 days' actual data and the next 10 days' forecasted\n",
    "data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
    "could significantly influence hospital operations. For each insight, you must develop a strategic action plan and clearly articulate the\n",
    "rationale behind each proposed action. This involves not only a deep understanding of the data but also a foresighted approach to foresee\n",
    "potential challenges and opportunities it indicates. Your action plans should focus on optimizing resource allocation, effectively managing\n",
    "staffing, and enhancing overall operational efficiency. Importantly, each action item must be accompanied by a justification, explaining\n",
    "why it is necessary and how it will positively impact the hospital's ability to provide high-quality patient care while adapting to the\n",
    "dynamic healthcare environment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "726d7116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240                       90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                      11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "\n",
      "[\"\\nYou are tasked with extracting actionable insights from the data received from the hospital's predictive dashboard and formulating\\ncorresponding action items. This critical role requires you to analyze the past 10 days' actual data and the next 10 days' forecasted\\ndata for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\\ncould significantly influence hospital operations. For each insight, you must develop a strategic action plan and clearly articulate the\\nrationale behind each proposed action. This involves not only a deep understanding of the data but also a foresighted approach to foresee\\npotential challenges and opportunities it indicates. Your action plans should focus on optimizing resource allocation, effectively managing\\nstaffing, and enhancing overall operational efficiency. Importantly, each action item must be accompanied by a justification, explaining\\nwhy it is necessary and how it will positively impact the hospital's abil\", 'ity to provide high-quality patient care while adapting to the\\ndynamic healthcare environment.\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text for Chunk 1:\n",
      "\n",
      "You are tasked with extracting actionable insights from the data received from the hospital's predictive dashboard and formulating\n",
      "corresponding action items. This critical role requires you to analyze the past 10 days' actual data and the next 10 days' forecasted\n",
      "data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
      "could significantly influence hospital operations. For each insight, you must develop a strategic action plan and clearly articulate the\n",
      "rationale behind each proposed action. This involves not only a deep understanding of the data but also a foresighted approach to foresee\n",
      "potential challenges and opportunities it indicates. Your action plans should focus on optimizing resource allocation, effectively managing\n",
      "staffing, and enhancing overall operational efficiency. Importantly, each action item must be accompanied by a justification, explaining\n",
      "why it is necessary and how it will positively impact the hospital's abilites.\n",
      "\n",
      "The hospital is a complex system, with many different types of hospital services. The hospital has a wide variety of services, including:\n",
      "...\n",
      ",\n",
      ":.,\n",
      " (1) Emergency Medicine, (2) Nursing, Nursing Home, Hospitality, Hospitals, Medical Services, Health Care, Healthcare Services\n",
      "(3) Health, Care Services (4) Hospital, Rehabilitation, Emergency Care\n",
      "1.1 The Hospital\n",
      "A hospital, or a hospital-based system of care, is an institution of higher quality than a traditional hospital. It is the primary care facility of a community. A hospital provides a high level of health care to the community, providing a quality of life for patients and staff. Hospitians are responsible for providing care for the health of patients, staff, patients' families, the general public, hospitals, nursing homes, rehabilitation facilities, hospital facilities and other facilities. Hospital-related services include: (a) the provision of medical services to patients; (b) medical care and rehabilitation services; and (c) other services that are provided by the patient. In addition, a patient's care is provided to a number of other patients. Patients are expected to be cared for by their caretakers, who are the caretaker of their own care. (See the section on \"The Hospital\" for more information.)\n",
      "2.2 The Health\n",
      "In the United States, there are approximately 1.5 million hospital beds. Approximately 1 in 5 patients in the U.S. have a chronic condition. There are about 1 million patients with chronic conditions in each of our hospitals. These patients are treated by physicians, nurses, pharmacists, dentists and others. They are cared and cared about by other people. Most of these patients have no known health problems. However, some patients may have serious health conditions. Some patients also have chronic diseases. Many of them have been diagnosed with a serious condition, such as cancer, heart disease, diabetes, cancer of any kind,\n",
      "Generated Text for Chunk 2:\n",
      "ity to provide high-quality patient care while adapting to the\n",
      "dynamic healthcare environment.\n",
      "\n",
      "The new system will be implemented in the following ways:\n",
      ". The new systems will provide a high level of quality care to patients, while providing a low level\n",
      "comprehensive care.. The system is designed to be flexible and adaptable to different patient needs. It will\n",
      ", therefore, be able to offer a wide range of services to all patients. This will allow for a variety of\n",
      "\"care options\" to suit different needs, and will also allow the system to adapt to changing patient\n",
      "\n",
      "\n",
      "Combined Generated Text:\n",
      "\n",
      "You are tasked with extracting actionable insights from the data received from the hospital's predictive dashboard and formulating\n",
      "corresponding action items. This critical role requires you to analyze the past 10 days' actual data and the next 10 days' forecasted\n",
      "data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
      "could significantly influence hospital operations. For each insight, you must develop a strategic action plan and clearly articulate the\n",
      "rationale behind each proposed action. This involves not only a deep understanding of the data but also a foresighted approach to foresee\n",
      "potential challenges and opportunities it indicates. Your action plans should focus on optimizing resource allocation, effectively managing\n",
      "staffing, and enhancing overall operational efficiency. Importantly, each action item must be accompanied by a justification, explaining\n",
      "why it is necessary and how it will positively impact the hospital's abilites.\n",
      "\n",
      "The hospital is a complex system, with many different types of hospital services. The hospital has a wide variety of services, including:\n",
      "...\n",
      ",\n",
      ":.,\n",
      " (1) Emergency Medicine, (2) Nursing, Nursing Home, Hospitality, Hospitals, Medical Services, Health Care, Healthcare Services\n",
      "(3) Health, Care Services (4) Hospital, Rehabilitation, Emergency Care\n",
      "1.1 The Hospital\n",
      "A hospital, or a hospital-based system of care, is an institution of higher quality than a traditional hospital. It is the primary care facility of a community. A hospital provides a high level of health care to the community, providing a quality of life for patients and staff. Hospitians are responsible for providing care for the health of patients, staff, patients' families, the general public, hospitals, nursing homes, rehabilitation facilities, hospital facilities and other facilities. Hospital-related services include: (a) the provision of medical services to patients; (b) medical care and rehabilitation services; and (c) other services that are provided by the patient. In addition, a patient's care is provided to a number of other patients. Patients are expected to be cared for by their caretakers, who are the caretaker of their own care. (See the section on \"The Hospital\" for more information.)\n",
      "2.2 The Health\n",
      "In the United States, there are approximately 1.5 million hospital beds. Approximately 1 in 5 patients in the U.S. have a chronic condition. There are about 1 million patients with chronic conditions in each of our hospitals. These patients are treated by physicians, nurses, pharmacists, dentists and others. They are cared and cared about by other people. Most of these patients have no known health problems. However, some patients may have serious health conditions. Some patients also have chronic diseases. Many of them have been diagnosed with a serious condition, such as cancer, heart disease, diabetes, cancer of any kind,ity to provide high-quality patient care while adapting to the\n",
      "dynamic healthcare environment.\n",
      "\n",
      "The new system will be implemented in the following ways:\n",
      ". The new systems will provide a high level of quality care to patients, while providing a low level\n",
      "comprehensive care.. The system is designed to be flexible and adaptable to different patient needs. It will\n",
      ", therefore, be able to offer a wide range of services to all patients. This will allow for a variety of\n",
      "\"care options\" to suit different needs, and will also allow the system to adapt to changing patient\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Your DataFrame containing historical data\n",
    "historical_data = {\n",
    "    'Date': [\n",
    "        '12/20/2020', '12/21/2020', '12/22/2020', '12/23/2020', '12/24/2020', '12/25/2020', '12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020',\n",
    "        '12/30/2020', '12/31/2020', '1/1/2021', '1/2/2021', '1/3/2021', '1/4/2021', '1/5/2021', '1/6/2021', '1/7/2021', '1/8/2021'\n",
    "    ],\n",
    "    'Admissions_Patient_Count': [95, 67, 88, 106, 96, 104, 98, 87, 105, 229, 231, 237, 202, 182, 207, 170, 174, 201, 226, 194],\n",
    "    'Discharge_Patient_Count': [68, 77, 83, 65, 84, 69, 79, 89, 83, 59, 60, 69, 71, 70, 75, 58, 65, 66, 61, 75],\n",
    "    'Transfer_Patient_Count': [7, 3, 5, 4, 5, 3, 8, 6, 6, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7]\n",
    "}\n",
    "\n",
    "historical_df = pd.DataFrame(historical_data)\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n",
    "\n",
    "# Slice data for the next 10 days (example forecast)\n",
    "next_10_days_forecast = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='1/9/2021', periods=10),\n",
    "    'Admissions_Patient_Count': [210, 220, 240, 250, 260, 270, 280, 290, 300, 310],\n",
    "    'Discharge_Patient_Count': [80, 85, 90, 95, 100, 105, 110, 115, 120, 125],\n",
    "    'Transfer_Patient_Count': [8, 9, 9, 10, 11, 11, 12, 12, 13, 13]\n",
    "})\n",
    "\n",
    "# Text block incorporating historical and forecast data\n",
    "input_text = f\"\"\"The first 10 records of historical patient counts are:\\n{historical_df.head(10).to_string(index=False)}\n",
    "\n",
    "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
    "\n",
    "The forecast for the next 10 days is:\\n{next_10_days_forecast.to_string(index=False)}\n",
    "\n",
    "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
    "\"\"\"\n",
    "print(input_text)\n",
    "\n",
    "# Persona of a hospital operations personnel\n",
    "#persona = \"As a hospital operations manager, I need insights on the patient counts to optimize resource allocation and staffing.\"\n",
    "\n",
    "# Prompt the model for insights or comments on the input text from the hospital operations point of view\n",
    "prompt_text = task #f\"{persona}\\n{input_text}\\nPlease provide insights or comments on the historical and forecasted patient counts from a hospital operations perspective:\"\n",
    "\n",
    "# Split the prompt into chunks to avoid exceeding token limit\n",
    "chunk_size = 1024  # Adjust as needed\n",
    "chunks = [prompt_text[i:i + chunk_size] for i in range(0, len(prompt_text), chunk_size)]\n",
    "\n",
    "generated_chunks = []\n",
    "print(chunks)\n",
    "# Generate text for each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate text for the chunk\n",
    "    output = model.generate(input_ids, max_length=600, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50,\n",
    "                            top_p=0.95)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_chunks.append(generated_text)\n",
    "\n",
    "    # Print generated text for this chunk\n",
    "    print(f\"Generated Text for Chunk {i + 1}:\")\n",
    "    print(generated_text)\n",
    "\n",
    "# Combine the generated chunks\n",
    "generated_text = \"\".join(generated_chunks)\n",
    "\n",
    "print(\"\\nCombined Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a52a0c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are tasked with extracting actionable insights from the data received from the hospital's predictive dashboard and formulating\n",
      "corresponding action items. This critical role requires you to analyze the past 10 days' actual data and the next 10 days' forecasted\n",
      "data for patient admissions, discharges, and transfers. Your aim is to identify key trends, patterns, and outliers in this data that\n",
      "could significantly influence hospital operations. For each insight, you must develop a strategic action plan and clearly articulate the\n",
      "rationale behind each proposed action. This involves not only a deep understanding of the data but also a foresighted approach to foresee\n",
      "potential challenges and opportunities it indicates. Your action plans should focus on optimizing resource allocation, effectively managing\n",
      "staffing, and enhancing overall operational efficiency. Importantly, each action item must be accompanied by a justification, explaining\n",
      "why it is necessary and how it will positively impact the hospital's abilites.\n",
      "\n",
      "The hospital is a complex system, with many different types of hospital services. The hospital has a wide variety of services, including:\n",
      "...\n",
      ",\n",
      ":.,\n",
      " (1) Emergency Medicine, (2) Nursing, Nursing Home, Hospitality, Hospitals, Medical Services, Health Care, Healthcare Services\n",
      "(3) Health, Care Services (4) Hospital, Rehabilitation, Emergency Care\n",
      "1.1 The Hospital\n",
      "A hospital, or a hospital-based system of care, is an institution of higher quality than a traditional hospital. It is the primary care facility of a community. A hospital provides a high level of health care to the community, providing a quality of life for patients and staff. Hospitians are responsible for providing care for the health of patients, staff, patients' families, the general public, hospitals, nursing homes, rehabilitation facilities, hospital facilities and other facilities. Hospital-related services include: (a) the provision of medical services to patients; (b) medical care and rehabilitation services; and (c) other services that are provided by the patient. In addition, a patient's care is provided to a number of other patients. Patients are expected to be cared for by their caretakers, who are the caretaker of their own care. (See the section on \"The Hospital\" for more information.)\n",
      "2.2 The Health\n",
      "In the United States, there are approximately 1.5 million hospital beds. Approximately 1 in 5 patients in the U.S. have a chronic condition. There are about 1 million patients with chronic conditions in each of our hospitals. These patients are treated by physicians, nurses, pharmacists, dentists and others. They are cared and cared about by other people. Most of these patients have no known health problems. However, some patients may have serious health conditions. Some patients also have chronic diseases. Many of them have been diagnosed with a serious condition, such as cancer, heart disease, diabetes, cancer of any kind,ity to provide high-quality patient care while adapting to the\n",
      "dynamic healthcare environment.\n",
      "\n",
      "The new system will be implemented in the following ways:\n",
      ". The new systems will provide a high level of quality care to patients, while providing a low level\n",
      "comprehensive care.. The system is designed to be flexible and adaptable to different patient needs. It will\n",
      ", therefore, be able to offer a wide range of services to all patients. This will allow for a variety of\n",
      "\"care options\" to suit different needs, and will also allow the system to adapt to changing patient\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b811f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75572ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240                       90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                      11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "\n",
      "\n",
      " As a hospital operations manager, I need insights on the patient counts to optimize resource allocation and staffing.\n",
      "The first 10 records of historical patient counts are:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2020-12-20                        95                       68                       7\n",
      "2020-12-21                        67                       77                       3\n",
      "2020-12-22                        88                       83                       5\n",
      "2020-12-23                       106                       65                       4\n",
      "2020-12-24                        96                       84                       5\n",
      "2020-12-25                       104                       69                       3\n",
      "2020-12-26                        98                       79                       8\n",
      "2020-12-27                        87                       89                       6\n",
      "2020-12-28                       105                       83                       6\n",
      "2020-12-29                       229                       59                       7\n",
      "\n",
      "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
      "\n",
      "The forecast for the next 10 days is:\n",
      "      Date  Admissions_Patient_Count  Discharge_Patient_Count  Transfer_Patient_Count\n",
      "2021-01-09                       210                       80                       8\n",
      "2021-01-10                       220                       85                       9\n",
      "2021-01-11                       240                       90                       9\n",
      "2021-01-12                       250                       95                      10\n",
      "2021-01-13                       260                      100                      11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "\n",
      "Please provide insights or comments on the historical and forecasted patient counts from a hospital operations perspective:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
      "             11\n",
      "2021-01-14                       270                      105                      11\n",
      "2021-01-15                       280                      110                      12\n",
      "2021-01-16                       290                      115                      12\n",
      "2021-01-17                       300                      120                      13\n",
      "2021-01-18                       310                      125                      13\n",
      "\n",
      "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Your DataFrame containing historical data\n",
    "historical_data = {\n",
    "    'Date': [\n",
    "        '12/20/2020', '12/21/2020', '12/22/2020', '12/23/2020', '12/24/2020', '12/25/2020', '12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020',\n",
    "        '12/30/2020', '12/31/2020', '1/1/2021', '1/2/2021', '1/3/2021', '1/4/2021', '1/5/2021', '1/6/2021', '1/7/2021', '1/8/2021'\n",
    "    ],\n",
    "    'Admissions_Patient_Count': [95, 67, 88, 106, 96, 104, 98, 87, 105, 229, 231, 237, 202, 182, 207, 170, 174, 201, 226, 194],\n",
    "    'Discharge_Patient_Count': [68, 77, 83, 65, 84, 69, 79, 89, 83, 59, 60, 69, 71, 70, 75, 58, 65, 66, 61, 75],\n",
    "    'Transfer_Patient_Count': [7, 3, 5, 4, 5, 3, 8, 6, 6, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7]\n",
    "}\n",
    "\n",
    "historical_df = pd.DataFrame(historical_data)\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n",
    "\n",
    "# Slice data for the next 10 days (example forecast)\n",
    "next_10_days_forecast = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='1/9/2021', periods=10),\n",
    "    'Admissions_Patient_Count': [210, 220, 240, 250, 260, 270, 280, 290, 300, 310],\n",
    "    'Discharge_Patient_Count': [80, 85, 90, 95, 100, 105, 110, 115, 120, 125],\n",
    "    'Transfer_Patient_Count': [8, 9, 9, 10, 11, 11, 12, 12, 13, 13]\n",
    "})\n",
    "\n",
    "# Text block incorporating historical and forecast data\n",
    "input_text = f\"\"\"The first 10 records of historical patient counts are:\\n{historical_df.head(10).to_string(index=False)}\n",
    "\n",
    "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
    "\n",
    "The forecast for the next 10 days is:\\n{next_10_days_forecast.to_string(index=False)}\n",
    "\n",
    "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
    "\"\"\"\n",
    "print(input_text)\n",
    "\n",
    "# Persona of a hospital operations personnel\n",
    "persona = \"As a hospital operations manager, I need insights on the patient counts to optimize resource allocation and staffing.\"\n",
    "\n",
    "# Prompt the model for insights or comments on the input text from the hospital operations point of view\n",
    "prompt_text = f\"{persona}\\n{input_text}\\nPlease provide insights or comments on the historical and forecasted patient counts from a hospital operations perspective:\"\n",
    "print('\\n', prompt_text)\n",
    "# Split the prompt into chunks to avoid exceeding token limit\n",
    "# (Your chunking code remains unchanged)\n",
    "\n",
    "generated_chunks = []\n",
    "# Generate text for each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    #input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "     # Ensure attention mask and set pad token ID\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    attention_mask = attention_mask.to(model.device)\n",
    "\n",
    "    # Generate text for the chunk\n",
    "    output = model.generate(input_ids, max_length=600, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_chunks.append(generated_text)\n",
    "\n",
    "# Combine the generated chunks\n",
    "generated_text = \"\".join(generated_chunks)\n",
    "\n",
    "# Extract insights related to resource allocation, staffing, and operations\n",
    "insights = generated_text.split(persona)[-1].strip()\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08951ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7126dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe791586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed784e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc653f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebe85a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your DataFrame containing historical data\n",
    "historical_data = {\n",
    "    'Date': [\n",
    "        '12/20/2020', '12/21/2020', '12/22/2020', '12/23/2020', '12/24/2020', '12/25/2020', '12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020',\n",
    "        '12/30/2020', '12/31/2020', '1/1/2021', '1/2/2021', '1/3/2021', '1/4/2021', '1/5/2021', '1/6/2021', '1/7/2021', '1/8/2021'\n",
    "    ],\n",
    "    'Admissions_Patient_Count': [95, 67, 88, 106, 96, 104, 98, 87, 105, 229, 231, 237, 202, 182, 207, 170, 174, 201, 226, 194],\n",
    "    'Discharge_Patient_Count': [68, 77, 83, 65, 84, 69, 79, 89, 83, 59, 60, 69, 71, 70, 75, 58, 65, 66, 61, 75],\n",
    "    'Transfer_Patient_Count': [7, 3, 5, 4, 5, 3, 8, 6, 6, 7, 6, 7, 7, 6, 7, 6, 6, 7, 7, 7]\n",
    "}\n",
    "\n",
    "historical_df = pd.DataFrame(historical_data)\n",
    "\n",
    "# Convert Date column to datetime type\n",
    "historical_df['Date'] = pd.to_datetime(historical_df['Date'])\n",
    "\n",
    "# Slice data for the next 10 days (example forecast)\n",
    "next_10_days_forecast = pd.DataFrame({\n",
    "    'Date': pd.date_range(start='1/9/2021', periods=10),\n",
    "    'Admissions_Patient_Count': [210, 220, 240, 250, 260, 270, 280, 290, 300, 310],\n",
    "    'Discharge_Patient_Count': [80, 85, 90, 95, 100, 105, 110, 115, 120, 125],\n",
    "    'Transfer_Patient_Count': [8, 9, 9, 10, 11, 11, 12, 12, 13, 13]\n",
    "})\n",
    "\n",
    "# Text block incorporating historical and forecast data\n",
    "input_text = f\"\"\"The first 10 records of historical patient counts are:\\n{historical_df.head(10).to_string(index=False)}\n",
    "\n",
    "These records represent the actual data of the number of patients getting admitted, discharged, and transferred in a hospital from the start to the 10th date.\n",
    "\n",
    "The forecast for the next 10 days is:\\n{next_10_days_forecast.to_string(index=False)}\n",
    "\n",
    "These forecasts represent the predicted number of patients to be admitted, discharged, and transferred in the hospital for the next 10 days.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe50a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt \u001b[38;5;241m+\u001b[39m chunk, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Generate insights based on the prompt and chunk\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1700\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Decode and append the generated insights\u001b[39;00m\n\u001b[0;32m     24\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:1490\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1487\u001b[0m         )\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1491\u001b[0m         input_ids,\n\u001b[0;32m   1492\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1493\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1494\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id,\n\u001b[0;32m   1495\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39meos_token_id,\n\u001b[0;32m   1496\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39moutput_scores,\n\u001b[0;32m   1497\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1498\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1500\u001b[0m     )\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:2233\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   2230\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2235\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2236\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2237\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2238\u001b[0m )\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2241\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1046\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:833\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    832\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m--> 833\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Prompt to generate insights on the input text\n",
    "prompt = f\"As a healthcare analyst, analyze the historical and forecasted patient counts provided below:\\n\\n{input_text}\\n\\nPlease provide insights or analysis on the trends, patterns, or noteworthy observations in these patient counts:\"\n",
    "\n",
    "# Split the input text into chunks to fit within the model's token limit\n",
    "max_token_limit = 1700 # tokenizer.model_max_length - len(tokenizer.encode(prompt, add_special_tokens=False)) - 10  # Some extra margin\n",
    "chunks = [input_text[i:i+max_token_limit] for i in range(0, len(input_text), max_token_limit)]\n",
    "\n",
    "generated_insights = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Tokenize the prompt and chunk of input text\n",
    "    input_ids = tokenizer.encode(prompt + chunk, return_tensors=\"pt\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "    # Generate insights based on the prompt and chunk\n",
    "    output = model.generate(input_ids, max_length=1700, num_return_sequences=1)\n",
    "\n",
    "    # Decode and append the generated insights\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_insights.append(generated_text)\n",
    "\n",
    "# Combine generated insights from chunks\n",
    "all_generated_insights = \" \".join(generated_insights)\n",
    "print(all_generated_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_generated_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7410d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Prompt to generate insights on the input text\n",
    "prompt = f\"Analyze the text:\\n\\n{input_text}\\n\\nPlease provide insights or analysis:\"\n",
    "\n",
    "# Split the input text into chunks to fit within the model's token limit\n",
    "max_token_limit = 1700 # tokenizer.model_max_length - len(tokenizer.encode(prompt, add_special_tokens=False)) - 10  # Some extra margin\n",
    "chunks = [input_text[i:i+max_token_limit] for i in range(0, len(input_text), max_token_limit)]\n",
    "\n",
    "generated_insights = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Tokenize the prompt and chunk of input text\n",
    "    input_ids = tokenizer.encode(prompt + chunk, return_tensors=\"pt\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "    # Generate insights based on the prompt and chunk\n",
    "    output = model.generate(input_ids, max_length=1700, num_return_sequences=1)\n",
    "\n",
    "    # Decode and append the generated insights\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_insights.append(generated_text)\n",
    "\n",
    "# Combine generated insights from chunks\n",
    "all_generated_insights = \" \".join(generated_insights)\n",
    "print(all_generated_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4306afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_generated_insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
